# MCP Chat - Modularer Chat-Client

Ein fortschrittlicher Chat-Client mit KI-Integration, Tool-Support und Echtzeit-Streaming √ºber Model Context Protocol (MCP).

## Features

- **ü§ñ Multi-AI Support**: OpenAI, Ollama, Anthropic, Custom AI-Provider
- **üîß Tool Integration**: SSE, HTTP-Stream, STDIO Protokolle
- **‚ö° Real-time Streaming**: Server-Sent Events f√ºr Live-AI-Antworten  
- **üîê Authentifizierung**: JWT-basierte Sicherheit
- **üé® Modern UI**: React, TypeScript, TailwindCSS
- **üê≥ Docker Support**: Vollst√§ndige Containerisierung
- **üìä Admin Panel**: Benutzer-, Tool- und AI-Verwaltung
- **üß™ Test Coverage**: Umfassende automatisierte Tests
- **üåê Dynamic CORS**: Flexible CORS-Konfiguration f√ºr verschiedene Deployment-Szenarien
- **‚öôÔ∏è Environment-aware**: Automatische Anpassung an Development/Production/Docker

## Tech Stack

### Frontend
- React 18 + TypeScript + Vite
- TailwindCSS f√ºr Styling
- Server-Sent Events f√ºr Streaming
- JWT Authentication

### Backend
- Node.js + Express
- SQLite Datenbank
- Real-time AI Streaming
- Tool- und KI-Adapter
- Comprehensive Logging

## Quick Start

```bash
# Clone repository
git clone <repository-url>
cd mcp-chat

# Start with Docker
docker-compose up -d

# Or run locally
npm install
npm run dev
```

## Streaming Architecture

Das System unterst√ºtzt Echtzeit-Streaming von AI-Antworten:

- **SSE Endpoint**: `/api/conversations/:id/stream`
- **Chunk-based Delivery**: Antworten werden in Echtzeit √ºbertragen
- **Multiple Providers**: Ollama und OpenAI Streaming-Support
- **Fallback Support**: Nicht-streaming Provider werden unterst√ºtzt

## AI Provider Configuration

### Ollama (Local)
```json
{
  "endpoint": "http://localhost:11434",
  "model": "llama3.2:latest"
}
```

### OpenAI
```json
{
  "apiKey": "sk-...",
  "model": "gpt-4",
  "temperature": 0.7,
  "maxTokens": 2048
}
```

## Default Login
- Username: `admin`
- Password: `admin`

## Development

```bash
# Backend
cd backend
npm install
npm run dev

# Frontend  
cd frontend
npm install
npm run dev

# Tests
npm test
```

## Docker Support

Die Anwendung l√§uft vollst√§ndig in Docker-Containern mit automatischer Netzwerk-Konfiguration f√ºr AI-Services.

## Configuration & Deployment

This project supports flexible configuration for different deployment scenarios:

### Environment Configuration

- **Development**: Uses `.env.local` files with localhost URLs
- **Docker**: Automatic service discovery with container names  
- **Production**: Environment variables for custom domains and secure settings

### CORS Configuration

The backend implements intelligent CORS handling:
- ‚úÖ Static allowed origins via `ALLOWED_ORIGINS` environment variable
- ‚úÖ Docker service name support (`frontend:*`, `backend:*`)
- ‚úÖ Development mode: automatic localhost port detection
- ‚úÖ Dynamic CORS headers based on request origin

### Quick Configuration Examples

```bash
# Local Development
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:5173
VITE_API_URL=http://localhost:3001/api

# Docker Development  
ALLOWED_ORIGINS=http://frontend:5173,http://localhost:3002
VITE_API_URL=http://backend:3001/api

# Production
ALLOWED_ORIGINS=https://myapp.com,https://admin.myapp.com
VITE_API_URL=https://api.myapp.com/api
JWT_SECRET=your-secure-production-secret
```

For detailed deployment instructions, see [DEPLOYMENT.md](./DEPLOYMENT.md).

## License

MIT License
